# this is the configuration file, change the entries here to configure how things run in
# train.py
# but should also be changable through terminal arguments

dataset:
  h5: squad_dataset.1.1.2.h5
  model_save_path: 'saved_models/model.pt'

scheduling:
  batch_size: 64
  valid_batch_size: 64
  epoch: 50
  enable_cuda: True  # disable this when running on machine without cuda
  cuda_seed: 42
  
optimizer:
  step_rule: 'adam'  # adam, sgd
  learning_rate: 0.002
  learning_rate_decay_ratio: 0.8  # if valid loss goes up, decay the learning rate
  learning_rate_decay_from_this_epoch: 0
  learning_rate_decay_patience: 2  # be patient
  learning_rate_cut_lowerbound: 0.01  # if lr < lowerbound * init_lr, then stop cutting
  clip_grad_norm: 5

global:
  fast_rnns: True
  dropout_between_rnn_layers: 0.3
  dropout_between_rnn_hiddens: 0.3
  dropout_in_rnn_weights: 0.2
  use_layernorm: True
  use_highway_connections: False

embedding:
  word_level:
    path: glove.6B.300d.txt.h5
    embedding_size: 300
    embedding_type: 'pretrained'  # pretrained, random
    embedding_trainable: False  # fix pretrained embedding
    embedding_dropout: 0.3
    embedding_word_dropout: 0.2
    embedding_oov_init: 'zero'  # zero, one, random

  char_level:
    enable: False
    embedding_size: 64
    embedding_rnn_size: [64]  # 16 on each direction
    embedding_trainable: True
    embedding_dropout: 0.3
    embedding_word_dropout: 0.

match_lstm:
  rnn_hidden_size: [300]  # 150 on each direction, use an empty list to disable preproc_rnn, if you want..
  match_lstm_hidden_size: [300]  # 150 on each direction
  decoder_hidden_size: 150  # uni directional rnn with 2 time steps
